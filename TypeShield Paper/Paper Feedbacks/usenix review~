Here are some thoughts from a quick first read. I'm happy to discuss the paper more thoroughly with you; but let me send you already the points below. Based on that, you can also tell me more specifically what type of feedback helps you the most.

The most important question to me is that of effectiveness. What would it mean for your tool to be more effective than existing solutions? Table 5 is very interesting in this regard. It shows that there are many call-sites that have hundreds of possible targets. The gap between TypeArmor/TypeShield and FCFI is large. It seems that an effective solution should reduce that gap.

Table 4 seems to indicate that type information alone will not significantly reduce the gap, even considering a perfect implementation. I think this means that we should look for other ways to match callsites to targets.

My hunch would be to manually look at a few callsites that have a high number of targets. Why do they have such a high target count? What could a good human analyst do to rule out these targets? When you found a few such rules for humans, you can then think about implementing them in a tool.

I believe you already have several ideas for future work, such as more fine-grained type lattices, distinguishing pointer types from value types, etc. These are good, but they will only bring your project to the theoretical optimum in Table 4, and that is "only" a few percent better than TypeArmor. Because of that, I would prioritize gathering ideas for other ways to reduce the number of legal call targets.

A few smaller questions:

- Your work depends on a specific calling convention. Could you explain it (show an example assembly snippet) and say which OSs and architectures use it?
- How does your actual check look like?
- Your liveness analysis seems interprocedural, but this is not quite clear. What do you mean by "if we choose to follow calls"? How would you handle recursive functions?
- Why are there so few callsites found (in Tables 1-3)? Doesn't a typical program have more callsites than call targets?

And one remark: when discussing related work, always be gentle. Remember that you're learning from them and improving on what they build. For example, I would not write that something "is not deployed in practice". First you don't know, and second your tool is not deployed either.

I've learned quite a lot already from reading your paper. I knew binary transformations are much harder than doing the same thing if source code is available. Your project offers one way to quantify this difference.

