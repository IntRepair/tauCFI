\section{Evaluation}
\label{chapter:Evaluation}
We evaluated \textsc{TypeShield} by instrumenting various open source applications and conducting a thorough analysis. Our test sample includes the two 
ftp server applications \textit{Vsftpd} (v.1.1.0) and \textit{Proftpd} (v.1.3.3), the two http server 
applications \textit{Postgresql} (v.9.0.10) and \textit{Mysql} (v.5.1.65), the memory cache application \textit{Memcached} (v.1.4.20) and the \textit{Node.js} 
server application (v.0.12.5). We chose these applications, which are a subset of the applications also used in the TypeAmor paper~\cite{veen:typearmor}, to 
allow for comparison. We addressed the following research questions (RQs).

\begin{itemize}[leftmargin=.12in]
 \item \textbf{RQ1:} How \textbf{precise} is \textsc{TypeShield}? (\cref{section:typeshieldprecision})
 \item \textbf{RQ2:} How \textbf{effective} is \textsc{TypeShield}? (\cref{section:typeshieldeffectiveness})
 \item \textbf{RQ3:} What is the \textbf{runtime overhead} of \textsc{TypeShield}? (\cref{section:typeshieldoverheadperformance})
 \item \textbf{RQ4:} What is \textsc{TypeShield's} \textbf{instrumentation overhead}? (\cref{section:typeshieldoverheadinstrumentation})
 \item \textbf{RQ5:} What \textbf{security level} does \textsc{TypeShield} offer? (\cref{RQ5: Security Analysis})
 \item \textbf{RQ6:} Which \textbf{upper bounds} can \textsc{TypeArmor} enforce? (\cref{RQ6:TyperArmor's Imprecise Parameter-Count Policy})
 \item \textbf{RQ7:} Is \textsc{TypeShield} \textbf{superior} compared to other tools? (\cref{RQ5: Is TypeShield better than other tools?})
\end{itemize}
\textbf{Comparison Method.} As we did not had access to the source code of TypeArmor during development of \textsc{TypeShield}
we implemented two modes in \textsc{TypeShield}. The first mode of our tool is a similar implementation of 
the \textit{count} policy described by TypeArmor. The second mode is our implementation of the \textit{type} policy on 
top of the \textit{count} policy implementation. 

\textbf{Experimental Setup.} Our used setup consisted in a VirtualBox (version 5.0.26r) instance, in which we ran a Kubuntu 16.04 LTS (Linux Kernel
version 4.4.0). We had access to 3GB of RAM and 4 out of 8 provided hardware threads (Intel i7-4170HQ @ 2.50 GHz).

\subsection{Precision}
\label{section:typeshieldprecision}
% \todo[inline]{In this section we need just one or two Table similar to what TypeArmor contains, first we need to define the fields which make most sense.}

To measure the precision of \textsc{TypeShield}, we need to compare the classification of callsites and calltargets as is given by our tool to some sort of ground 
truth for our test targets. We generate this ground truth by compiling our test targets using a 
custom compiled Clang/LLVM compiler (v.4.0.0 trunk 283889) with a MachineFunction pass inside the x86 code generation implementation of LLVM. We essentially collect
three data points for each callsite/calltarget from our LLVM-pass:
\textit{1)} the point of origination, which is either the name of the calltarget or the name of the function the callsite resides in, 
\textit{2)} the return type that is either expected by the callsite or provided by the calltarget, and 
\textit{3)} the parameter list that is provided by the callsite or expected by the calltarget, which discards the variadic argument list.

However, before we can proceed to measure the quality and precision of \textsc{TypeShield}'s classification of calltargets and callsites using ground truth, we 
need to evaluate the quality and applicability of the ground truth, we collected.

\subsubsection{Quality and Applicability of Ground Truth}
\label{subsection:typeshieldprecision}
We assessed the applicability of our collected ground truth, by 
\begin{table}[h!]
\resizebox{\columnwidth}{!}{
	\begin{tabular}{l|r|r|r|r|r|r}%
	\toprule
	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{3}{c|}{ {\bfseries calltargets}} & \multicolumn{3}{c}{{\bfseries callsites} }\\
	\bfseries Target & match & Clang miss &  tool miss &  match & Clang miss & tool miss% specify table head
	\\\midrule
	\csvreader[before filter=\ifthenelse{\equal{\csvcoli}{geomean}}{\csvfilterreject}{\csvfilteraccept},  late after line=\\, late after last line=\\\midrule]{csvs/matching.O2.csv}{
		%1=\target, 2=\opt, 3=\fns, 4=\fnsnotClang, 5=\fnsnotpadyn, 6=\ats, 7=\atnotClang, 8=\atnotpadyn, 9=\cscount, 10=\csClang, 11=\cspadyn
	}
	{\csvcoli & \csvcoliii & \csvcoliv \ (\csvcolv \%)& \csvcolvi \ (\csvcolvii \%)& \csvcolxiii & \csvcolxiv  \ (\csvcolxv) & \csvcolxvi  \ (\csvcolvii) }% specify your coloumns here

	\csvreader[before filter=\ifthenelse{\equal{\csvcoli}{geomean}}{\csvfilteraccept}{\csvfilterreject},  late after line=\\, late after last line=\\\bottomrule]{csvs/matching.O2.csv}{
		%1=\target, 2=\opt, 3=\fns, 4=\fnsnotClang, 5=\fnsnotpadyn, 6=\ats, 7=\atnotClang, 8=\atnotpadyn, 9=\cscount, 10=\csClang, 11=\cspadyn
	}
	{\textit{\csvcoli} & \csvcoliii & \csvcoliv \ (\csvcolv \%)& \csvcolvi \ (\csvcolvii \%)& \csvcolxiii & \csvcolxiv \ (\csvcolxv) & \csvcolxvi \ (\csvcolvii) }% specify your coloumns here
    	\end{tabular}
    	
    	}
%     	}
	\caption {Table shows the quality of structural matching provided by our automated verify and test environment, 
	regarding callsites and calltargets when compiling with optimization level O2. The label Clang miss 
	denotes elements not found in the data-set of the Clang/LLVM pass. The label tool miss denotes elements not found in the data-set of \textsc{TypeShield}. 
% 	\textcolor{red}{TODO-add more description in order to indicate the advantage of our tool. What geomean values are good, low or hig? What is the main thing which can be observed if 
% 	looking at this table?}
        }
	\label{tbl:matchingquality}
\end{table}
assessing the structural compatibility of our two data sets. First, we investigate the comparability of calltargets. Second, we check the compatibility of callsites. The 
results are depicted in Table \ref{tbl:matchingquality}.

\textbf{Calltargets.} The obvious choice for structural comparison regarding calltargets is their name, as these are simply functions. First, we have to remove internal 
functions from our datasets like the \texttt{\_init} or \texttt{\_fini} functions, which are of no relevance for this investigation. Furthermore, while C functions can
simply be matched by their name as they are unique through the binary, the same cannot be said about the language C++. One of the key differences between C and C++ is 
function overloading, which allows defining several functions with the same name, as long as they differ in namespace or parameter type. 
As LLVM does not know about either concept, the Clang compiler needs to generate unique names. The method used for unique name generation is called mangling and composes
the actual name of the function, its return type, its name-space and the types of its parameter list. Therefore, we need to reverse this process and then compare the fully
typed names. Table \ref{tbl:matchingquality} shows three data points regarding calltargets for the optimization level O2:
\textit{1)} Number of comparable calltargets that are found in both datasets, 
\textit{2)} Clang miss: Number of calltargets that are found by \textsc{TypeShield}, but not by our Clang/LLVM pass, and 
\textit{3)} Tool miss: Number of calltargets that are found by our Clang/LLVM pass, but not by \textsc{TypeShield}

The problematic column is the Clang miss column, as these values might indicate problems with \textsc{TypeShield}. These numbers are relatively low (below 1\%) with only Node.js
showing a noticeable higher value than the rest (around 1.6\%). The column labeled tool miss lists higher numbers, however, these are of no real concern to us, as our ground truth 
pass possibly collects more data: All source files used during the compilation of our test-targets are incorporated into our ground truth. The compilation might generate more than
one binary and therefore, not necessary all source files are used for our test-target.

Considering this, we can state with confidence that our structural matching between ground truth and \textsc{TypeShield} regarding calltargets is nearly perfect (above 98\%).

\textbf{Callsites.} While our structural matching of calltargets is rather simple, the matter of matching callsites is more complex. Our tool can provide accurate addressing of 
callsites within the binary. However, Clang/LLVM does not have such capabilities in its intermediate representation (IR). Furthermore, the IR is not the final representation within
the compiler, as the IR is transformed into a machine-based representation (MR), which is again optimized. Although, we can read information regarding parameters from the IR, it 
is not possible with the MR. Therefore, we extract that data directly after the conversion from IR to MR and read that data at the end of the compilation. To not unnecessarily 
pollute our dataset, we only considered calltargets, which have been found in both datasets. Table \ref{tbl:matchingquality} shows three data points regarding callsites for 
the optimization level O2:
\textit{1)} Number of comparable callsites that are found in both datasets,
\textit{2)} Clang miss: Number of callsites that are discarded from the data set of \textsc{TypeShield}, and
\textit{3)} Tool miss: Number of callsites that are discarded from the data set of our Clang/LLVM pass.

Both columns (Clang miss and Tool miss) show a relatively low number of problems (< 0.5\%). Therefore, we can also 
safely state that our structural matching between ground truth and \textsc{TypeShield} regarding callsites is close to perfection (above 99\%).

\subsubsection{Classification Precision (\textit{count})}
\label{subsection:typeshieldcountprecision}
We measured two data points per target, the number and ratio of perfect 
\begin{table}[h!]
\resizebox{\columnwidth}{!}{
	\begin{tabular}{l|r|r|r|r|r|r}%
	\toprule
	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{3}{c}{\bfseries Calltargets} & \multicolumn{3}{c}{\bfseries Callsites}\\
	
	\bfseries Target & \#  &  perfect args &  perfect return & \# & perfect args &  perfect return % specify table head
	\\\midrule
	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilterreject}{\csvfilteraccept}, late after line=\\, late after last line=\\\midrule]{csvs/classification_count.O2.csv}{
		%1=opt,2=target,3=cs,4=cs args,5=perfect,6=cs args,7=problem,8 = cs non-void ,9=correct,10 = cs non-void, 11=problem,12 = ct, 13 = ct args, 14=perfect, 15 = ct args, 16=problem, 17 = ct void, 18=correct, 19=ct void, 20=problem
}
	{\csvcolii  &  \csvcolxii & \csvcolxiii \ (\csvcolxiv \%) & \csvcolxvii \ (\csvcolxviii \%) & \csvcoliii & \csvcoliv \ (\csvcolv \%) & \csvcolviii \ (\csvcolix\%)}% specify your coloumns here

	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilteraccept}{\csvfilterreject}, late after line=\\, late after last line=\\\bottomrule]{csvs/classification_count.O2.csv}{
		%1=opt,2=target,3=cs,4=cs args,5=perfect,6=cs args,7=problem,8 = cs non-void ,9=correct,10 = cs non-void, 11=problem,12 = ct, 13 = ct args, 14=perfect, 15 = ct args, 16=problem, 17 = ct void, 18=correct, 19=ct void, 20=problem
}
	{\textit{\csvcolii}  &  \csvcolxii & \csvcolxiii \ (\csvcolxiv \%) & \csvcolxvii \ (\csvcolxviii \%) & \csvcoliii & \csvcoliv \ (\csvcolv \%) & \csvcolviii \ (\csvcolix\%)}% specify your coloumns here
    	\end{tabular}
}

\caption {The results for classification of callsites and calltargets using our \textit{count} policy on the O2 optimization level, when comparing to the ground truth obtained by our Clang/LLVM pass.
The label perfect args denotes all occurrences when our result and the ground truth perfectly match regarding the required/provided arguments. The label perfect return denotes this for return values. 
% 		\textcolor{red}{TODO-add more description in order to indicate the advantage of our tool. What geomean values are good, low or hig? What is the main thing which can be observed if 
% 	        looking at this table?}
                }
		\label{tbl:precisionCOUNT}
\end{table}
classifications and the number and ratio of problematic classifications, which in the case of calltargets refers to overestimations and in case of callsites refers to underestimations 
(see Table~\ref{tbl:precisionCOUNT} for more details).
\textbf{Experiment Setup (Calltargets).} Union combination operator with an $analyze$ function that follows into occurring direct calls.
\textbf{Results (Calltargets).} The problem rate is under 0.01\%, as there are only two test targets, that exhibit a problematic classification. 
The rate of perfect classification is in general over 80\% with Mysql as an exception (73.85\%) resulting in a geometric mean of 86.86\%.
\textbf{Experiment Setup (Callsites).} Union combination operator with an $analyze$ function that does not follow into occurring direct calls while relying on a backward inter-procedural analysis.
\textbf{Results (Callsites).} The problem rate is under 0.01\%, as there is only one test target, that exhibits a problematic classification. The rate of perfect classification is in 
general over 60\% with Nginx (48.49\%) 
and Node.js (56.34\%) as exceptions resulting in a geometric mean of 71.97\%.


\subsubsection{Classification Precision (\textit{type})}
\label{subsection:typeshieldcountprecision}
We measured two data points per test target, the number and ratio of perfect classifications 
\newline
\begin{table}[h!]
\resizebox{\columnwidth}{!}{
	\begin{tabular}{l|r|r|r|r|r|r}%

	\toprule
	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{3}{c}{\bfseries Calltargets} & \multicolumn{3}{c}{\bfseries Callsites}\\
	
	\bfseries Target & \#  &  perfect  args&  perfect return& \# & perfect args &  perfect return% specify table head
	\\\midrule
	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilterreject}{\csvfilteraccept},  late after line=\\, late after last line=\\\midrule]{csvs/classification_type.O2.csv}{
		%1=opt,2=target,3=cs,4=cs args,5=perfect,6=cs args,7=problem,8 = cs non-void ,9=correct,10 = cs non-void, 11=problem,12 = ct, 13 = ct args, 14=perfect, 15 = ct args, 16=problem, 17 = ct void, 18=correct, 19=ct void, 20=problem
}
	{\csvcolii  &  \csvcolxii & \csvcolxiii \ (\csvcolxiv \%) & \csvcolxvii \ (\csvcolxviii \%) & \csvcoliii & \csvcoliv \ (\csvcolv \%) & \csvcolviii \ (\csvcolix\%)}% specify your coloumns here

	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilteraccept}{\csvfilterreject},  late after line=\\, late after last line=\\\bottomrule]{csvs/classification_type.O2.csv}{
		%1=opt,2=target,3=cs,4=cs args,5=perfect,6=cs args,7=problem,8 = cs non-void ,9=correct,10 = cs non-void, 11=problem,12 = ct, 13 = ct args, 14=perfect, 15 = ct args, 16=problem, 17 = ct void, 18=correct, 19=ct void, 20=problem
}
	{\textit{\csvcolii}  &  \csvcolxii & \csvcolxiii \ (\csvcolxiv \%) & \csvcolxvii \ (\csvcolxviii \%) & \csvcoliii & \csvcoliv \ (\csvcolv \%) & \csvcolviii \ (\csvcolix\%)}% specify your coloumns here
    	\end{tabular}

}
	\caption {The result for classification of callsites using our \textit{type} policy on the O2 optimization level, when comparing to the ground truth obtained by our Clang/LLVM pass. The label perfect args denotes all occurrences when our result and the ground truth perfectly match regarding the required/provided arguments. The label perfect return denotes this for return values.
% 	\textcolor{red}{TODO-add more description in order to indicate the advantage of our tool. What geomean values are good, low or hig? What is the main thing which can be observed if 
% 	looking at this table?}
        }
	\label{tbl:precisionTYPE}
\end{table}
and the number and ratio of problematic classifications, which in the case of calltargets refers to overestimations and in case of callsites refers to underestimations. The results are depicted 
in Table \ref{tbl:precisionTYPE}.
\textbf{Experiment Setup (Calltargets).} Union combination operator with an $analyze$ function that does follow into occurring direct calls and a vertical merge that intersects all reads until 
the first write.
\textbf{Results (Calltargets).} For half of the set, the problem rate is under 1\% and for the other half it is not above 10\%, resulting in a geomean of 1.92\%. The rate of perfect classification
is in general over 70\% with Nginx (69.38\%) and Mysql (63.16\%) resulting in a geometric mean of 77.15\%.
\textbf{Experiment Setup (Callsites).} Union combination operator with an $analyze$ function that does not follow into occurring direct calls while relying on a backward inter-procedural analysis.
\textbf{Results (Callsites).} For two thirds of the set, the problem rate is under 2\% and for the last third it is not above 10\%, resulting in a geomean of 1.38\%. The rate of perfect 
classification is in general over 50\% with Node.js (44.76\%) as an exception resulting in a geometric mean of 68.35\%.


%
%
%Efficiency
%
%
\subsection{Effectiveness}
\label{section:typeshieldeffectiveness}
% \todo[inline]{In this section we need just one or two Table similar to what TypeArmor contains, 
% first we need to define the fields which make most sense.}

\begin{table*}[htbp!]
\begin{center}
 \resizebox{2\columnwidth}{!}{
	\begin{tabular}{l|r|rcl|r|rcl|r|rcl|r|rcl|r}%

	\toprule
	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{1}{c}{\bfseries AT} & \multicolumn{4}{c}{\bfseries \textit{count}*} & \multicolumn{4}{c}{\bfseries \textit{count}} & \multicolumn{4}{c}{\bfseries \textit{type}*} & \multicolumn{4}{c}{\bfseries \textit{type}}\\
	
	\bfseries Target && \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median & \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median & \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median & \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median  % specify table head
	\\\midrule
	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilterreject}{\csvfilteraccept},  late after line=\\, late after last line=\\\midrule]{csvs/policy_compare_at.O2.csv}{
	%1=opt,2=target,3=at,4=count safe avg,5=count safe sig,6=count safe median,7=count prec avg,8=count prec sig,9=count prec median,10=count* avg,11=count* sig,12=count* median,13=type safe avg,14=type safe sig,15=type safe median,16=type prec avg,17=type prec sig,18=type prec median,19=type* avg,20=type* sig,21=type* median
 }
	{\csvcolii  &  \csvcoliii & \csvcolx & $\pm$ & \csvcolxi & \csvcolxii & \csvcolvii & $\pm$ & \csvcolviii& \csvcolix& \csvcolxix & $\pm$ & \csvcolxx& \csvcolxxi & \csvcolxvi & $\pm$ & \csvcolxvii& \csvcolxviii }% specify your coloumns here

	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilteraccept}{\csvfilterreject},  late after line=\\, late after last line=\\\bottomrule]{csvs/policy_compare_at.O2.csv}{
	%1=opt,2=target,3=at,4=count safe avg,5=count safe sig,6=count safe median,7=count prec avg,8=count prec sig,9=count prec median,10=count* avg,11=count* sig,12=count* median,13=type safe avg,14=type safe sig,15=type safe median,16=type prec avg,17=type prec sig,18=type prec median,19=type* avg,20=type* sig,21=type* median
 }
	{\textit{\csvcolii}  &  \csvcoliii & \csvcolx & $\pm$ & \csvcolxi & \csvcolxii & \csvcolvii & $\pm$ & \csvcolviii& \csvcolix& \csvcolxix & $\pm$ & \csvcolxx& \csvcolxxi & \csvcolxvi & $\pm$ & \csvcolxvii& \csvcolxviii }% specify your coloumns here

    	\end{tabular}

}
	\caption {Restriction results of allowed callsites per calltarget for several test series on various targets compiled with 
	Clang using optimization level O2. Note that the basic restriction to address taken only calltargets (see column AT) is 
	present for each other series. The label $count^*$ denotes the best possible reduction using our \emph{count} policy 
	based on the ground truth collected by our Clang/LLVM pass, while $count$ denotes the results of our implementation of 
	the \emph{count} policy derived from the binaries. The same applies to $type*$ and $type$ regarding the \emph{type} policy. 
	A lower number of calltargets per callsite indicates better results. Note that our \emph{type} policy is superior to 
	the \emph{count} policy, as it allows for a stronger reduction of allowed calltargets. 
	We consider this a good result which further improves the state-of-the-art. 
	Finally, we provide the median and the pair of mean and standard deviation to allow for a 
	better comparison with other state-of-the-art tools.
% 	\textcolor{red}{TODO-add more description in order to indicate the advantage of our tool. What geomean values are good, low or hig? What is the main thing which can be observed if 
% 	looking at this table?}
        }
	\label{tbl:policycompat}
\end{center}
\end{table*}

We are now going to evaluate the effectiveness of \textsc{TypeShield} leveraging the result of several experiment runs: First, we are going to establish a baseline using the data collected from our Clang/LLVM pass, which are the theoretical limits our implementation can reach for both the \textit{count} and the \textit{type} schema. Second, we are going to evaluate the effectiveness of our \textit{count} policy. Third, we are going to evaluate the effectiveness of our \textit{type} policy. For each series, we collected three data points per test target, the average number of calltargets per callsite, the standard deviation $\sigma$ and the median. 
The results are depicted in Table \ref{tbl:policycompat}. 

\subsubsection{Theoretical Limits.}
\label{subsection:theoreticallimit}
We explore the theoretical limits regarding the effectiveness of the \textit{count} and \textit{type} policies by relying on the collected ground truth data, essentially assuming perfect classification.
\textbf{Experiment Setup.} Based on the type information collected by our Clang/LLVM pass, we conducted two experiment series.
We derived the available number of calltargets for each callsite based on the collected ground truth applying the \textit{count} and \textit{type} schemes.
\textbf{Results.}
\textit{1)} The theoretical limit of the \textit{count*} schema has a geometric mean of 233 possible calltargets, which is 16.48\% of the geometric mean of the total available calltargets, and
\textit{2)} The theoretical limit of the \textit{type*} schema has a geometric mean of 210 possible calltargets, which is 14.86\% of the geometric mean of the total available calltargets.
When compared, the theoretical limit of the \textit{type} policy allows about 10\% less available calltargets in the geomean in O2 than the limit of the \textit{count} policy.

\subsubsection{Reduction achieved by \textsc{TypeShield}}
\label{subsection:typeshieldvslimitcount}
\textbf{Experiment Setup.} We set up our two experiment series based on our previous evaluations regarding the classification precision for the \textit{count} and the \textit{type} policy.
\textbf{Results.}
\textit{1)}  The \textit{count} schema has a geometric mean of 315 possible calltargets, which is 22.29\% of the geometric mean of total available calltargets. This is 35.19\% more than the theoretical limit of available calltargets per callsite, and
\textit{2)}  The \textit{type} schema has a geometric mean of 290 possible calltargets, which is 20.52\% of the geometric mean of total available calltargets. This is 38.09\% more than the theoretical limit of available calltargets per callsite.
When compared, our implementation of the \textit{type} policy allows about 7.93\% less available calltargets in the geomean in O2 than our implementation of the \textit{type} policy.


%\begin{table}[h!]
%\resizebox{.4\textwidth}{!}{
%	\begin{tabular}{l|c|rcl|c|rcl|c}%
%
%	\toprule
%	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{1}{c}{\bfseries AT} & \multicolumn{4}{c}{\bfseries \textit{count} safe} & \multicolumn{4}{c}{\bfseries \textit{count} prec}\\
%	
%	\bfseries Target && \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median & \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median  % specify table head
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\midrule]{../MA_Pictures/policy_compare_at.O2.csv}{
%	%1=opt,2=target,3=at,4=count safe avg,5=count safe sig,6=count safe median,7=count prec avg,8=count prec sig,9=count prec median,10=count* avg,11=count* sig,12=count* median,13=type safe avg,14=type safe sig,15=type safe median,16=type prec avg,17=type prec sig,18=type prec median,19=type* avg,20=type* sig,21=type* median
% }
%	{\csvcolii  &  \csvcoliii & \csvcoliv & $\pm$ & \csvcolv & \csvcolvi & \csvcolvii & $\pm$ & \csvcolviii& \csvcolix}% specify your coloumns here
%
%    	\end{tabular}
%}
%		\caption {The results of comparing \textit{count} safe and precision implementation restricted using an address taken analysis throughout different optimizations.}
%		\label{tbl:policycompatcount}
%\end{table}
%
%\begin{table}[h!]
%\resizebox{.4\textwidth}{!}{
%	\begin{tabular}{l|c|rcl|c|rcl|c}%
%
%	\toprule
%	\multicolumn{1}{c}{\bfseries O2} & \multicolumn{1}{c}{\bfseries AT} & \multicolumn{4}{c}{\bfseries \textit{type} safe} & \multicolumn{4}{c}{\bfseries \textit{type} prec}\\
%	
%	\bfseries Target && \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median & \multicolumn{3}{c}{ limit (mean $\pm$ $\sigma$)} & median  % specify table head
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\midrule]{../MA_Pictures/policy_compare_at.O2.csv}{
%	%1=opt,2=target,3=at,4=count safe avg,5=count safe sig,6=count safe median,7=count prec avg,8=count prec sig,9=count prec median,10=count* avg,11=count* sig,12=count* median,13=type safe avg,14=type safe sig,15=type safe median,16=type prec avg,17=type prec sig,18=type prec median,19=type* avg,20=type* sig,21=type* median
% }
%	{\csvcolii  &  \csvcoliii & \csvcolxiii & $\pm$ & \csvcolxiv & \csvcolxv & \csvcolxvi & $\pm$ & \csvcolxvii& \csvcolxviii}% specify your coloumns here
%
%    	\end{tabular}
%}
%		\caption {The results of comparing \textit{type} safe and precision implementation restricted using an address taken analysis throughout different optimizations.}
%		\label{tbl:policycompattype}
%\end{table}

%
%\newpage
%\section{Security Analysis of \textsc{TypeShield}}
%\label{section:typeshieldsecurityanalysis}
%
%In this section, we discuss how effective \textsc{TypeShield} is 
%stopping advance code-reuse attacks (CRAs).
%Patching Policies
%Two types of diagrams. Table 5 from TypeArmor and a CDF to compare param count and param type. (baseline)
%here we put the CDF graphs from. There is no accurate security metrics to asses the security level of the enforced policy.
%
%
%
%\begin{table}
%\centering
%\resizebox{0.8\textwidth}{!}{
%	\begin{tabular}{l|c|c|c|c|c|c|c|c|c}%
%	\toprule
%	\multicolumn{1}{c}{\bfseries O0} & \multicolumn{1}{c}{} & \multicolumn{8}{|c}{ {\bfseries parameters}} \\
%	\bfseries Target & \bfseries \#CS & \bfseries -x & \bfseries +0 & \bfseries +1 & \bfseries +2 & \bfseries +3 & \bfseries +4 & \bfseries +5 & \bfseries +6 % specify table head
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\midrule]{../MA_Pictures/classification_cs.O0.csv}{
%		%1=target,2=opt,3=cs,4=problems,5=+0,6=+1,7=+2,8=+3,9=+4,10=+5,11=+6,12=non-void-ok,13=non-void-problem
%	}
%	{\csvcoli & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii & \csvcolviii & \csvcolix & \csvcolx & \csvcolxi }% specify your coloumns here
%
%	\multicolumn{1}{c}{\bfseries O1}
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\midrule]{../MA_Pictures/classification_cs.O1.csv}{
%		%1=target,2=opt,3=cs,4=problems,5=+0,6=+1,7=+2,8=+3,9=+4,10=+5,11=+6,12=non-void-ok,13=non-void-problem
%	}
%	{\csvcoli & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii & \csvcolviii & \csvcolix & \csvcolx & \csvcolxi}% specify your coloumns here
%
%	\multicolumn{1}{c}{\bfseries O2}
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\midrule]{../MA_Pictures/classification_cs.O2.csv}{
%		%1=target,2=opt,3=cs,4=problems,5=+0,6=+1,7=+2,8=+3,9=+4,10=+5,11=+6,12=non-void-ok,13=non-void-problem
%	}
%	{\csvcoli & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii & \csvcolviii & \csvcolix & \csvcolx & \csvcolxi}% specify your coloumns here
%
%	\multicolumn{1}{c}{\bfseries O3}
%	\\\midrule
%	\csvreader[ late after line=\\, late after last line=\\\bottomrule]{../MA_Pictures/classification_cs.O3.csv}{
%		%1=target,2=opt,3=cs,4=problems,5=+0,6=+1,7=+2,8=+3,9=+4,10=+5,11=+6,12=non-void-ok,13=non-void-problem
%	}
%	{\csvcoli & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii & \csvcolviii & \csvcolix & \csvcolx & \csvcolxi}% specify your coloumns here
%
%
%    	\end{tabular}
%	}
%		\caption {Table shows the overestimation of the parameter count in matched callsites occurring in our precision focussed implementation of the \textit{count} policy, with -x denoting problematic callsites, when compiling with optimization levels O0 through O3}
%	\label{tbl:baselinecs}
%\end{table}
%
%\begin{figure}
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Vsftpd.pdf}
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/lighttpd.pdf}\\
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Memcached.pdf}
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Mysql.pdf}\\
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Nginx.pdf}
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Node.js.pdf}\\
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Postgresql.pdf}
%\includegraphics[width=0.5\textwidth]{../MA_Pictures/Proftpd.pdf}
%\end{figure}
%

\subsection{Runtime Overhead}
\label{section:typeshieldoverheadperformance}
% \todo[inline]{In this section we need one or two Table similar to what TypeArmor contains, first we need to define the fields which make most sense.}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/spec_cpu2006.pdf}
    \caption{SPEC CPU2006 Benchmark Results.
%     \textcolor{red}{TODO-add more description in order to indicate the advantage of our tool. What geomean values are good, low or hig? replace with our values. remove this stub.}
    }
%     \vspace{-.5cm}
    \label{fig:awesome_image}
\end{figure}

Figure~\ref{fig:awesome_image} depicts the runtime overhead obtained by applying our tool to several SPEC CPU2006 benchmarks
in count mode and in count and type mode, respectively.
The obtained geomean runtime overhead is around 4\% performance overhead when instrumenting using DynInst. 
One reason for the performance drop includes cache misses introduced by jumping between the old and the new executable section 
of the binary generated by duplicating and patching. This is necessary, because when
outside of the compiler, it is nearly impossible to relocate indirect control flow. Therefore, 
every time an indirect control flow occurs, one jumps into the old executable section and from 
there back to the new executable section. Moreover, this is also dependent on the actual structure 
of the target, as it depends on the number of indirect control flow operations per time unit.
Another reason for the slightly higher (yer acceptable) performance overhead is due to our
runtime policy which is more complex than that of other state-of-the-art tools.
Finally, it can be observed that our runtime overhead is comparable (4\%) than state-of-the-art tools (around 3\%), 
thus qualifying \textsc{TypeShield} as a highly practical tool.

\todo[inline]{add a binary patch that does not crash none of the programs from SPEC CPU2006.}
\todo[inline]{need a Table with all the results for each of the SPEC CPU2006 programs and a bar diagram}

\subsection{Instrumentation Overhead}
\label{section:typeshieldoverheadinstrumentation}

\todo[inline]{here we need a bar chart, see TypeArmor paper.}
\todo[inline]{Measure the size (in bytes) of the SPEC2006 testes in RQ3 before and after adding all the patches}

The instrumentation overhead (\textit{i.e.,} binary blow-up) or the change in size due to patching is mostly due to the method DynInst uses to patch binaries. 
Essentially, the executable part of the binary is duplicated and extended with the patch. The usual ratio we encountered in our experiments is 
around 40\% to 60\% with Postgres having an increase of 150\% in binary size. One cannot reduce that 
value significantly, because of the nature of code relocation after loosing the information which a compiler has. Especially indirect control flow 
changes are very hard to relocate. Therefore, instead each important basic block in the old code contains a jump instruction to the new position of the basic block.

\subsection{Security Analysis}
\label{RQ5: Security Analysis}
% \subsubsection{CDF Analysis}
% \label{CDF Analysis}

\begin{figure}[h] 
% \vspace{-.45cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \resizebox{1.04\columnwidth}{!}{\includesvg{svgs/postgresqlO2}}
%     \vspace{-.7cm}
    \caption{Postgresql -O2 CDF.} 
    \label{fig7} 
    \vspace{1ex}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \resizebox{1.04\columnwidth}{!}{\includesvg{svgs/mysqlO2}} 
    \caption{MySQL.js -O2 CDF.} 
    \label{fig8} 
    \vspace{1ex}
  \end{minipage} 
%   \begin{minipage}[b]{0.5\linewidth}
%     \centering
%     \resizebox{1.04\columnwidth}{!}{\includesvg{proftpdO2.svg}}
%     \caption{Proftpd -O2 CDF.} 
%     \label{fig9} 
%     \vspace{1ex}
%   \end{minipage}%% 
%   \begin{minipage}[b]{0.5\linewidth}
%     \centering
%     \resizebox{1.04\columnwidth}{!}{\includesvg{mysqlO2}} 
%     \caption{Mysql -O2 CDF.} 
%     \label{fig10} 
%     \vspace{1ex}
%   \end{minipage} 
\end{figure}
% Figures~\ref{fig7}, \ref{fig8}, \ref{fig9}, and \ref{fig10} 
Figures~\ref{fig7} and~\ref{fig8}
depict the CDFs of the Postgresql and MySQL programs which were compiled with the -O2 Clang compiler flag. 
We selected these two programs randomly from our sample programs. 
The CDFs depict the number of legal callsite targets and the difference between the type and the count policies. 
While the count  policies have only a few changes, the number of changes that can be seen within the 
type policies is vastly higher. The reason for that is fairly straightforward: the number of buckets 
that are used to classify the callsites and calltargets is simply higher. While type policies mostly 
perform better than the count policies, there are still parts within the type plot that are above the 
count plot, the reason for that is also relatively simple: the maximum number of calltargets a 
callsite can access has been reduced. Therefore, a lower number of calltargets is a higher 
percentage than before. However, Figure~\ref{fig8} depicts also very clearly
that the \textit{count*} and \textit{type*} have lower values as 
\textit{count} and \textit{type}, respectively. This further, confirms our assumptions 
w.r.t. these used metrics. Finally, note that the results dependent on the particular 
internal structure of the hardened program.

%REMOVED THIS TODO FROM PAPER: todo. Also, add the buckets diagram, see Figure 9 in the typearmor paper.

\subsection{TyperArmor's Parameter-Count Policy}
\label{RQ6:TyperArmor's Imprecise Parameter-Count Policy}
% \textbf{Permissive Parameter-Count-Based Policies.}
\label{Too Permissive Parameter-Based Policies}
TypeArmor~\cite{veen:typearmor} enforces a CFI-based runtime policy in a binary for constraining object dispatches at the callsite based on
function parameter count checks. The authors argue that their policy reports only an \textit{overestimation} for the parameters prepared by a 
callsite and \textit{underestimation} for the number of parameters consumed by the matching calltargets. 
The authors suggest that their technique is effective against COOP attacks. 

We do not fully agree with this claim and, furthermore, we believe that their callsite vs. calltarget set enforcing policy is too permissive and thus
many potential indirect forward edge based control flow transfers are possible. Consider the following example. In the best case for each callsite 
preparing, say, $p=4 \in [1, 6]$ parameters their policy could theoretically allow only the calltargets which consume the same number as parameters as
prepared, $c=4 \in [1, 6]$. Note that this does not hold due to 
the aforementioned callsite overestimation and calltarget underestimation, thus all possible numerical mismatches are allowed by their policy as long
as $p$ is greater or equal to $c$.

\begin{itemize}
[leftmargin=.12in]
\item TypeArmor \textbf{\textit{ideally}} would allow for a single callsite a set of calltargets containing a maximum of $117649$ possibilities if we 
consider the maximum value of provided parameters to be $p=6$ (due to $p \in [1, 6]$ possible provided parameters). Now, consider 7 C++ integer parameter
types $t$: $int$, $char$, $unsigned char$, $bool$, $long$, $unsigned long$, and $short$. Thus, we obtain $t^{p}=7^{6}=117649$ allowed calltargets per 
callsite if TypeArmor is used. Note that for simplicity reasons we considered $t=7$ but in practice $t$ is often even larger since there are many types
of parameters in C++. The complete list of fundamental C++ types contains 20 types; not including data structures or object types. Thus, all these data 
types would be ignored by TypeArmor. Also, note that all other callsites having more than 6 parameters would be not checked by TypeArmor as well.

\item TypeArmor \textbf{\textit{actually}} allows more than $t^{p}$ calltargets per callsite. If we have $t=7$ integer types due to TypeArmors overestimation
and underestimation we get for each callsite an additional number of calltargets. Let $p=6$, then we get $c = 6x + 5y+ 4z + 3t + 2p + 1v$ where:
$x$ is the sum of all calltargets consuming 6 parameters, 
$y$ is the sum of all calltargets consuming 5 parameters 
and so on down to 0 parameters. Note that this holds since TypeArmor allows more parameters to be provided than consumed by the calltarget.
Then, $c = 2100 = 600 + 500 + 400 + 300 + 200 + 100 \ iff \ x=y=z=t=p=v=100$. 
Note that $x=100$ is feasible under realistic conditions in large applications (\textit{i.e.,} Google Chrome, Firefox). 
Next $2100$ is added to $7^{6}$. Thus, for a single callsite providing $p=6$ parameters TypeArmor allows theoretically in 
total $7^{6} + 2100 = 1197496$ calltargets for each callsite.
Similar reasoning applies to $p=5$ where we get $7^{5} + (1500 = 500 + 400 + 300 + 200 + 100) = \ 18307 \ iff \ x=y=z=t=p=v=100$ 
allowed calltarget per callsite, or $p \in [1, 4]$, too.
\end{itemize}

Finally, as TypeArmor is too permissive we present \textsc{TypeShield} which deals with the variable type state explosion due to different parameter types 
by considering an approximation (note that alias analysis and thus type analysis in binaries is undecidable~\cite{alias:undecidable}) of parameter types 
based on register width. Consequently, the allowed calltarget set for each callsite is drastically reduced.

\subsection{Comparison with Other Tools}
\label{RQ5: Is TypeShield better than other tools?}
% \vspace{-.15cm}
\begin{table}[h]
\resizebox{\columnwidth}{!}{
	\begin{tabular}{l|r|r|r|r|r}%
	\toprule
	\bfseries Target & AT & TypeArmor  &  IFCC &  TypeShield (count) & TypeShield (type)% specify table head
	\\\midrule
	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilterreject}{\csvfilteraccept},  late after line=\\, late after last line=\\\midrule]{csvs/tools_compare.csv}{
		%1=\target, 2=\opt, 3=\fns, 4=\fnsnotClang, 5=\fnsnotpadyn, 6=\ats, 7=\atnotClang, 8=\atnotpadyn, 9=\cscount, 10=\csClang, 11=\cspadyn
	}
	{\csvcolii & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii}% specify your coloumns here

	\csvreader[before filter=\ifthenelse{\equal{\csvcolii}{geomean}}{\csvfilteraccept}{\csvfilterreject},  late after line=\\, late after last line=\\\bottomrule]{csvs/tools_compare.csv}{
		%1=\target, 2=\opt, 3=\fns, 4=\fnsnotClang, 5=\fnsnotpadyn, 6=\ats, 7=\atnotClang, 8=\atnotpadyn, 9=\cscount, 10=\csClang, 11=\cspadyn
	}
	{\textit{\csvcolii} & \csvcoliii & \csvcoliv & \csvcolv & \csvcolvi & \csvcolvii}% specify your coloumns here
    	\end{tabular}}
%     	}
	\caption {Medians of calltargets per callsite for different tools.} 
	\label{tbl:toolcompare}
% 	\vspace{-.5cm}
\end{table}

Table~\ref{tbl:toolcompare} depicts~\footnote{Note that the smaller the geomean numbers are,
	the better the technique is. AT is a technique which allows calltargets that are address taken. 
	IFCC is a compiler based solution and depicted here as a reference for what is possible when 
	source code is available. TypeArmor and TypeShield on the other hand are binary-based tools. 
	We can observe that our type-based tool reduces the number of calltargets by up to 35\% when 
	compared to the AT method and on average by 13\% when comparing with TypeArmor. 
% 	Note that we removed Nginx out of the table
% 	because at the time of our evaluation it produced outlier results which we further wanted to investigate.
	Finally, we think that in addition by tweaking of our analysis we can further reduce the calltarget set for each callsite.
% 	Thus, increasing even more the offered security benefit.
}
a comparison between \textsc{TypeShield}, TypeArmor and IFCC with respect to the count of calltargets per callsites. 
The values depicted in this table for TypeArmor and IFCC are taken from the original TypeArmor paper.
We compare our version of address taken analysis (AT), TypeArmor, TypeShield (count), TypeShield (type) and IFCC. The first 
thing to notice is that when comparing these values, one can see that we did not depicted a separation based on return type or the 
CFC that TypeArmour introduced. Therefore, when implementing those measures, we think that our solution would improve even 
more with respect to precision than TypeArmor. While we anticipate that it is possible to surpass TypeArmor implementing those two solutions 
in our tool, we deem it nearly impossible to be able to compete with IFCC, which can directly operate on the source code level 
and has access to more possibilities than simply inspecting function parameters or return values.
