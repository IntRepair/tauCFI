\section{Discussion}
\label{chapter:Discussion}

\textbf{Comparison with TypeArmor.}
\label{section:comptype}
We are looking at two sets of results. First of all, we compare the overall precision of our implementation
of the COUNT policy with the results from TypeArmor to set the perspective for the precision of our TYPE 
policy. We cannot compare data regarding overestimations of calltargets or underestimations of callsites, 
as TypeArmor did not provide sufficient data. The second point of comparison is the reduction of calltargets
per callsite, however, this comparison is rather crude, as we most surely do not have the same measuring
environment and not sufficient data to infer its quality.

\textit{Precision of Classification.}
TypeArmor reports a geometric mean of 83.26\% for the perfect classification of calltargets regarding 
parameter count in optimization level O2, which compares rather well to our result of 82.24\%. Regarding
the perfect classification of callsites we report a geometric mean of 81.6\% perfect classification 
regarding parameter count, while TypeArmor reprots a geometric mean of 79.19\%. Howevver we also have
a geometric mean of about 7\% regarding underestimations in the callsite classification with an upper
bound of 16\%, while TypeArmor reports that it does not incur underestimations in their callsites.
Now, for our type based classification we incur the cost for two error sources. First, the error from
the parameter count classification, which we base our type analysis on and second for the type analysis
itself. The numbers for the perfect classification of calltargets regarding parameter types we report a
72.25\% geometric mean of perfect classification, which is 87.85\% of our precision regarding parameter
counts. However we report a geometric mean of 57.36\%
for perfect classification of callsites, which altough seemingly low, is still 69.74\% of our precsion
regarding parameter counts.

\textit{Reduction of Available Calltargets}
While our count based precision focused implementation achieves a reduction in the same ballpark as
TypeArmour regarding our test targets, lets us believe that our implementation of their classification
schema is a sufficient approximation to compare against. However, we cannot safely compare those numbers,
as the information regarding their test environment are rather sparse and the only data available is the
median, which in our opinion does discard valuable information from the actual result set. This is the
main reason we implemented an approximation, because we needed more metrics to compare \textit{TypeShield}
and TypeArmor regarding calltargets. Using average and sigma, we can report that our precision focused
type based classification can reduce the number of calltargets, by up to 20\% more than parameter number
based classification with an overall reduction of about 9\%.


\textbf{TypeArmor Discrepancies.}
\label{section:discrep}
As we have no access to source code of TypeArmor, we habe implemented an approximation
of TypeArmor. Using this approximation we found some discrepancies between the data that we collected
and data that was presented.
A minor discrepancy between our results and the results of TypeArmor is that, while they basically implemented
what we call a destructive merge operator for the liveness analysis. However, our data suggestes that this
operator is marginally inferior to the union pathmerge operator, when we compared them in our implementation.
A major concern is the classification of calltargets, while we were able to reduce the number of overerstimations
of calltargets regarding parameter counts to essentially 0, the number of underestimations of calltarget did
stay at a geometric mean of 7\%. This error rate is rather large when compared to the reported 0\% underestimation
of TypeArmor, however we are not entirely sure what has caused this discrepancy. A possibility is the differing
test environments, or a bug within our implementation that we are not aware of, or simply reaching defintions
analysis alone is not the best possible algorithm for this particular problem.

\textbf{Improving \textit{TypeShield}.}
\label{section:venuesimp}
To improve our type analysis, we see atleast two possibilities. Incorporating refined dataflow analysis and 
expanding the scope to also include memory. The main point of improvement is not the precision but for now 
more importantly the reduction of underestimations in the callsite analysis.

To refine the dataflow analysis, we propse the actual tracking of data values and simple operations, as these
can be used to better differentiate the actual wideness stored within the current register. The highest gain, 
we see here would be the establishment of upper and lower bounds regarding values within the register, which 
would allow for more sophisticated callsite and calltarget invariants. Essentially we would have to resort 
to symbolic execution or some other sort of precise abstract interpretation.

Expanding the scope to also include memory, is another possible way of improving the type analysis, as it 
would allow us to distinguish normal 32 or 64 bit values and pointer addresses. Although we already have a 
limited approach of that in our reaching implementation, we still see room for improvement, as we only check
whether a value is within one of three binary sections or 0.

\textbf{Limitations of \textit{TypeShield}.}
\label{section:limit}
First of all, we are limited by the capabilities of the DynInst Instrumentation Environment, the main problem,
we are facing here is that non returning functions like exit are not detected reliably in some cases, which is
why we were not able to test the Pure-FTP server, as it heavily relies on these functions. The problem is that
those non returning functions usually appear as a second branch within a function that occurs after the normal
control flow, causing basic blocks from the following function to be attributed to the current function. This
results in a malformed control flow graph and erroneous attribution of callsites and problematic misclassifications
for both calltargets and callsites.

Another limitation of \textit{TypeShield} is it reliance on variety within the binary, in particular we rely on
the fact that functions use more than only 64bit values or pointers within their parameter list. Should this
scenario occur, our analysis has nothing to work with and essentially degrades into a parameter count based
implementation. Thankfully this occurrence is quite rare, as we experienced within our experiments. When working
based on source level information, we could not detect a difference between our TYPE and a COUNT policies. 
However when leveraging our tool, we were able to detect differences, which reinforces the fact, that we do 
not rely on declaration of parameters but usage of those.
